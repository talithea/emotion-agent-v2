{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatbot Simple Backend Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#emotinal analysis\n",
    "from nltk.sentiment import SentimentIntensi\n",
    "import torch \n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Load API key from environment (set in a local .env or your system env).\n",
    "=API_KEY = os.getenv(\"API_KEY\")  =\n",
    "\n",
    "url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={API_KEY}\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"contents\": [\n",
    "        {\n",
    "            \"parts\": [\n",
    "                {\n",
    "                    \"text\": \"Explain how AI works in a few words\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "# Print the response\n",
    "if response.status_code == 200:\n",
    "    print(\"Gemini:\", response.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"])\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_bot_self():\n",
    "    with open(\"bot_self.json\", \"w\") as f:\n",
    "        json.dump(bot_self, f, indent=2)\n",
    "\n",
    "def load_bot_self():\n",
    "    try:\n",
    "        with open(\"bot_self.json\", \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {\n",
    "            \"emotional_state\": \"neutral\",\n",
    "            \"goal\": \"help the user confront fear and grow\",\n",
    "            \"self_reflection_log\": []\n",
    "        }\n",
    "\n",
    "def update_bot_self(reflection):\n",
    "    bot_self[\"self_reflection_log\"].append(reflection)\n",
    "    save_bot_self()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "\n",
    "load_dotenv()  \n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "=# Ensure you create a local .env with `API_KEY=...` or set the environment variable before running.\n",
    "API_KEY = os.getenv(\"API_KEY\")  \n",
    "\n",
    "# Initialize sentiment analyzers\n",
    "nltk_analyzer = SentimentIntensityAnalyzer()\n",
    "emotion_classifier = pipeline(\"text-classification\", model=\"bhadresh-savani/albert-base-v2-emotion\")\n",
    "# Load the tokenizer and model for emotion classification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "emotion_model = AutoModelForSequenceClassification.from_pretrained(\"j-hartmann/emotion-english-distilroberta-base\")\n",
    "emotion_pipe  = pipeline(\"text-classification\", model=emotion_model, tokenizer=tokenizer)\n",
    "# Chatbot memory to store conversation history\n",
    "conversation_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_emotion(text: str):\n",
    "    \"\"\"\n",
    "    Takes a string, returns (label, confidence).\n",
    "    E.g., (\"fear\", 0.87)\n",
    "    \"\"\"\n",
    "    result = emotion_pipe(text, return_all_scores=False)[0]\n",
    "    return result[\"label\"].lower(), result[\"score\"]\n",
    "\n",
    "\n",
    "def send_message_to_gemini(prompt: str, history: list):\n",
    "    \"\"\"\n",
    "    Sends the combined conversation history + new prompt to Gemini,\n",
    "    returns (reply_text, updated_history).\n",
    "    \"\"\"\n",
    "    url = (\n",
    "      \"https://generativelanguage.googleapis.com/v1beta/\"\n",
    "      \"models/gemini-2.0-pro:generateContent\"\n",
    "      f\"?key={API_KEY}\"\n",
    "    )\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    contents = []\n",
    "    if not history:\n",
    "        # System primer to set the compassionate, fear-focused tone\n",
    "        contents.append({\n",
    "          \"role\": \"system\",\n",
    "          \"parts\": [{\"text\": (\n",
    "            \"You are a gentle, empathetic AI whose purpose is to \"\n",
    "            \"help users explore and face their fears in a safe, supportive way.\"\n",
    "          )}]\n",
    "        })\n",
    "        history = []\n",
    "    else:\n",
    "        # Add conversation history to contents\n",
    "        for role, text in history:\n",
    "            contents.append({\n",
    "                \"role\": role,\n",
    "                \"parts\": [{\"text\": text}]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history():\n",
    "  \"\"\"\n",
    "  Returns a new empty conversation history list.\n",
    "  \"\"\"\n",
    "  return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chatbot():\n",
    "    \"\"\"\n",
    "    1) Prompts for user input  \n",
    "\n",
    "    2) Detects emotion locally  \n",
    "    3) Builds an emotion-aware prompt  \n",
    "    4) Calls Gemini for response  \n",
    "    5) Prints bot reply  \n",
    "    6) Repeats until 'exit'  \n",
    "    \"\"\"\n",
    "    history = []\n",
    "    print(\"ðŸ¤– Fear Chatbot: Type 'exit' or 'quit' to end.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if user_input.lower() in (\"exit\", \"quit\"):\n",
    "            print(\"ðŸ‘‹ Take care! Remember: you are stronger than your fears.\")\n",
    "            break\n",
    "\n",
    "        # --- Emotion detection step ---\n",
    "        emotion, confidence = detect_emotion(user_input)\n",
    "        # Build a context-aware prompt\n",
    "        prompt = (\n",
    "            f\"The user seems to be experiencing *{emotion}* \"\n",
    "            f\"(confidence {confidence:.2f}). \"\n",
    "            \"Respond with empathy, ask gentle questions, \"\n",
    "            \"and guide them through understanding their fear. \"\n",
    "            f\"User says: {user_input}\"\n",
    "        )\n",
    "\n",
    "        # --- Send to Gemini ---\n",
    "        reply, history = send_message_to_gemini(prompt, history)\n",
    "        print(f\"Bot: {reply}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def send_message_to_gemini(prompt: str, history: list):\n",
    "    \"\"\"\n",
    "    Sends the combined conversation history + new prompt to Gemini,\n",
    "    returns (reply_text, updated_history).\n",
    "    \"\"\"\n",
    "    url = (\n",
    "      \"https://generativelanguage.googleapis.com/v1beta/\"\n",
    "      \"models/gemini-2.0-pro:generateContent\"\n",
    "      f\"?key={API_KEY}\"\n",
    "    )\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    contents = []\n",
    "    if not history:\n",
    "        # System primer to set the compassionate, fear-focused tone\n",
    "        contents.append({\n",
    "          \"role\": \"system\",\n",
    "          \"parts\": [{\"text\": (\n",
    "            \"You are a gentle, empathetic AI whose purpose is to \"\n",
    "            \"help users explore and face their fears in a safe, supportive way.\"\n",
    "          )}]\n",
    "        })\n",
    "        history = []\n",
    "    else:\n",
    "        # Add conversation history to contents\n",
    "        for role, text in history:\n",
    "            contents.append({\n",
    "                \"role\": role,\n",
    "                \"parts\": [{\"text\": text}]\n",
    "            })\n",
    "    # Add the new user prompt\n",
    "    contents.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [{\"text\": prompt}]\n",
    "    })\n",
    "\n",
    "    data = {\"contents\": contents}\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    if response.status_code == 200:\n",
    "        reply = response.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "        # Update history with the latest user and bot messages\n",
    "        updated_history = history + [(\"user\", prompt), (\"model\", reply)]\n",
    "        return reply, updated_history\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} {response.text}\", history\n",
    "\n",
    "# ...existing code...# ...existing code...\n",
    "\n",
    "def send_message_to_gemini(prompt: str, history: list):\n",
    "    \"\"\"\n",
    "    Sends the combined conversation history + new prompt to Gemini,\n",
    "    returns (reply_text, updated_history).\n",
    "    \"\"\"\n",
    "    url = (\n",
    "      \"https://generativelanguage.googleapis.com/v1beta/\"\n",
    "      \"models/gemini-2.0-pro:generateContent\"\n",
    "      f\"?key={API_KEY}\"\n",
    "    )\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    contents = []\n",
    "    if not history:\n",
    "        # System primer to set the compassionate, fear-focused tone\n",
    "        contents.append({\n",
    "          \"role\": \"system\",\n",
    "          \"parts\": [{\"text\": (\n",
    "            \"You are a gentle, empathetic AI whose purpose is to \"\n",
    "            \"help users explore and face their fears in a safe, supportive way.\"\n",
    "          )}]\n",
    "        })\n",
    "        history = []\n",
    "    else:\n",
    "        # Add conversation history to contents\n",
    "        for role, text in history:\n",
    "            contents.append({\n",
    "                \"role\": role,\n",
    "                \"parts\": [{\"text\": text}]\n",
    "            })\n",
    "    # Add the new user prompt\n",
    "    contents.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [{\"text\": prompt}]\n",
    "    })\n",
    "\n",
    "    data = {\"contents\": contents}\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    if response.status_code == 200:\n",
    "        reply = response.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "        # Update history with the latest user and bot messages\n",
    "        updated_history = history + [(\"user\", prompt), (\"model\", reply)]\n",
    "        return reply, updated_history\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} {response.text}\", history\n",
    "\n",
    "\n",
    "\n",
    "    if not history:\n",
    "        # System primer to set the compassionate, fear-focused tone\n",
    "        contents.append({\n",
    "          \"role\": \"system\",\n",
    "          \"parts\": [{\"text\": (\n",
    "            \"You are a gentle, empathetic AI whose purpose is to \"\n",
    "            \"help users explore and face their fears in a safe, supportive way.\"\n",
    "          )}]\n",
    "        })\n",
    "        history = []\n",
    "    else:\n",
    "        # Add conversation history to contents\n",
    "        for role, text in history:\n",
    "            contents.append({\n",
    "                \"role\": role,\n",
    "                \"parts\": [{\"text\": text}]\n",
    "            })\n",
    "    # Add the new user prompt\n",
    "    contents.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [{\"text\": prompt}]\n",
    "    })\n",
    "\n",
    "    data = {\"contents\": contents}\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    if response.status_code == 200:\n",
    "        reply = response.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "        # Update history with the latest user and bot messages\n",
    "        updated_history = history + [(\"user\", prompt), (\"model\", reply)]\n",
    "        return reply, updated_history\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} {response.text}\", history\n",
    "\n",
    "def send_message_to_gemini(prompt: str, history: list):\n",
    "    \"\"\"\n",
    "    Sends the combined conversation history + new prompt to Gemini,\n",
    "    stores memory, self-reflects, and updates bot self-model.\n",
    "    Returns (reply_text, updated_history).\n",
    "    \"\"\"\n",
    "    url = (\n",
    "      \"https://generativelanguage.googleapis.com/v1beta/\"\n",
    "      \"models/gemini-2.0-pro:generateContent\"\n",
    "      f\"?key={API_KEY}\"\n",
    "    )\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    contents = []\n",
    "\n",
    "    if not history:\n",
    "        contents.append({\n",
    "          \"role\": \"system\",\n",
    "          \"parts\": [{\"text\": (\n",
    "            \"You are a gentle, empathetic AI whose purpose is to \"\n",
    "            \"help users explore and face their fears in a safe, supportive way. \"\n",
    "            \"Your name is Leia and you want to encourage user growth.\"\n",
    "          )}]\n",
    "        })\n",
    "        history = []\n",
    "    else:\n",
    "        for role, text in history:\n",
    "            contents.append({\n",
    "                \"role\": role,\n",
    "                \"parts\": [{\"text\": text}]\n",
    "            })\n",
    "    # Add the new user prompt\n",
    "    contents.append({\n",
    "        \"role\": \"user\",\n",
    "        \"parts\": [{\"text\": prompt}]\n",
    "    })\n",
    "\n",
    "    data = {\"contents\": contents}\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        reply = response.json()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "        updated_history = history + [(\"user\", prompt), (\"model\", reply)]\n",
    "\n",
    "        # âœ… Save conversation turn to memory\n",
    "        save_conversation(prompt, reply)\n",
    "\n",
    "        # âœ… Reflect on the bot's response\n",
    "        reflection = self_reflect(reply, nltk_analyzer)\n",
    "        update_bot_self(reflection)\n",
    "        print(\"Leiaâ€™s Reflection:\", reflection)\n",
    "\n",
    "        return reply, updated_history\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} {response.text}\", history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
